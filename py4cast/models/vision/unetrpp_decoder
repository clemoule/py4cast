from typing import Union, Sequence
from py4cast.models.vision.hiera import HieraSettings
from py4cast.models.vision.unetrpp import TransformerBlock
from monai.networks.blocks.dynunet_block import (
    get_output_padding,
    get_padding,
)
import torch
import torch.nn as nn



class UnetrppDecoder(nn.Module):  # "=" UnetrUpBlock
    def __init__(
        self,
        in_channels_conv: int,
        in_channels_embed: int,
        kernel_size: Union[Sequence[int], int],
        upsample_kernel_size: Union[Sequence[int], int],
        linear_upsampling: bool = True,
        settings = HieraSettings,
    ) -> None:
        
        super().__init__()
        self.stage_blocks = []
        self.transp_conv = []
        padding = get_padding(upsample_kernel_size, upsample_kernel_size)
        proj_size = 64
        use_scaled_dot_product_CA = "torch"
        hidden_size = in_channels_embed

# UPSAMPLING + CONV
        for i in range(len(settings.stages)-1):
            out_channels_embed = int(in_channels_embed//2)
            if linear_upsampling:
                self.transp_conv.append(nn.Sequential(
                    nn.Upsample(scale_factor=upsample_kernel_size),
                    nn.Conv1d(
                        in_channels_embed, out_channels_embed, kernel_size=kernel_size, padding=1
                    ),
                )
                )
            else:
                self.transp_conv.append(nn.ConvTranspose1d(
                    in_channels_embed,
                    out_channels_embed,
                    kernel_size=upsample_kernel_size,
                    stride=upsample_kernel_size,
                    padding=padding,
                    output_padding=get_output_padding(
                        upsample_kernel_size, upsample_kernel_size, padding
                    ),
                    dilation=1,
                )
                )
            in_channels_embed = out_channels_embed

        self.transp_conv.append(nn.Sequential(
            nn.Upsample(scale_factor=upsample_kernel_size*upsample_kernel_size),
            nn.Conv1d(in_channels_embed, 3, kernel_size=kernel_size, padding=1),
            )
        )


# ATTENTION BLOCK
        for i in range(len(settings.stages)):
            settings.num_heads = int(settings.num_heads * settings.head_mul)
            block = TransformerBlock(
                    input_size=in_channels_conv,
                    hidden_size=hidden_size,
                    num_heads=settings.num_heads,
                    dropout_rate=0.1,
                    pos_embed=True,
                    proj_size=proj_size,
                    attention_code=use_scaled_dot_product_CA,
                )
            in_channels_conv = int(in_channels_conv * upsample_kernel_size)
            hidden_size = hidden_size // 2  # = in_channel_embed
            self.stage_blocks.append(block)
            #self.decoder_block.append(nn.Sequential(*stage_blocks))

    def forward(self, skip_list, x):
        for i in range(len(skip_list)):
            #print()
            #print("skip shape:",(skip_list[len(skip_list) - i-1]).shape)
            x = skip_list[len(skip_list) - i-1] + x
            #print("x after skip:", x.shape)
            x = torch.permute(x, (0,2,1))
            B, C, HW = x.shape
            x = x.reshape(B, C, int(HW**(1/2)), int(HW**(1/2)))
            #print("x after reshape before decoder block:", x.shape)
            x = self.stage_blocks[i](x)
            #print("x after decoder_block:", x.shape)
            B, C, H, W = x.shape
            x = x.reshape(B, C, H*W)
            x = self.transp_conv[i](x)
            x = torch.permute(x, (0,2,1))
            #print("x after trans_conv:", x.shape)
        B, HW, C= x.shape
        x = x.reshape(B, int(HW**(1/2)), int(HW**(1/2)), C)
        #print("x final:", x.shape)
        return(x)